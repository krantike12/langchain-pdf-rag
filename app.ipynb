{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad3dc2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Desktop\\lang_pdf\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate \n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aee365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "Secret_key = os.getenv(\"Google_key\")\n",
    "SERPER_API_KEY = os.getenv(\"Serper_Key_LangChain\")\n",
    "#print(SERPER_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f79d44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, it is **not possible to determine if an 80% ROI can be attained.**\n",
      "\n",
      "Here's why:\n",
      "\n",
      "1.  **Redshift App Context:** This section details the technical configuration for SAML 2.0 sign-on methods, Okta integration, and application settings. It contains no financial information, costs, benefits, or metrics related to return on investment.\n",
      "2.  **Report on Recent Workshops and Conferences:** This report outlines the necessity, impact, and beneficiaries of two workshops aimed at strengthening India's public library system. While it discusses the positive impact on literacy, access to knowledge, and the potential for policy changes, it does **not** provide any financial data, project costs, or quantifiable monetary benefits that would allow for an ROI calculation. The benefits described are qualitative and societal.\n",
      "\n",
      "To calculate ROI, you would need information on:\n",
      "*   The **cost** of the \"plan\" (which isn't clearly defined in financial terms, but refers to the workshops and potentially the redshift app setup).\n",
      "*   The **monetary benefits** or savings generated by the plan.\n",
      "\n",
      "Neither of these is present in the provided text.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#input for Pdf\n",
    "pdf_input = input(\"Enter the path to the PDF file: \")\n",
    "#Load pdf \n",
    "pdf_Loader = PyPDFLoader(pdf_input)\n",
    "\n",
    "#Split pdf into chunks\n",
    "\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\\n\" ,  chunk_size= 1000, chunk_overlap=100)\n",
    "\n",
    "\n",
    "response = pdf_Loader.load()\n",
    "page_content = response[0].page_content\n",
    "\n",
    "hasText = ''\n",
    "for content in response:\n",
    "    if content.page_content.strip():  # Check if there's any non-whitespace text\n",
    "        \n",
    "        hasText = \"Text found in the PDF.\"\n",
    "        break\n",
    "    else:\n",
    "        hasText = \"No text found in the PDF.\"\n",
    "        print(hasText)\n",
    "        print(\"Moving to OCR process or alternative handling.\")\n",
    "        unstructured_loader = UnstructuredPDFLoader(pdf_input, strategy=\"ocr_only\")\n",
    "        ocr_response = unstructured_loader.load()\n",
    "        ocr_content = \"\".join([ocr_response.page_content for ocr_response in ocr_response])\n",
    "        print(ocr_content)\n",
    "\n",
    "\n",
    "\n",
    "splitted_text = text_splitter.split_text(page_content or ocr_content)\n",
    "\n",
    "\n",
    "\n",
    "#Embed the chunks using HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "#vector store using Chroma\n",
    "db = Chroma.from_texts(texts=splitted_text, \n",
    "                       embedding=embeddings, \n",
    "                       collection_name=\"my_collection\")\n",
    "\n",
    "query = input(\"Enter your question: \")\n",
    "response_query = db.similarity_search(query, k=3)\n",
    "\n",
    "#web search using Serper API\n",
    "web_search = GoogleSerperAPIWrapper(serper_api_key=SERPER_API_KEY)  \n",
    "#web_search.run(query)\n",
    "\n",
    "#Context building\n",
    "context = \"\\n\".join([doc.page_content for doc in response_query])\n",
    "web_search_results = web_search.run(query)\n",
    "combined_context = f\"{context}\\n\\n and Web Search Results:\\n{web_search_results}\"\n",
    "\n",
    "# print(response_query)\n",
    "\n",
    "#prompt template\n",
    "\n",
    "sys_template = \"\"\"You are a helpful assistant that Read and Analyse PDF Files. \n",
    "Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know\"\"\"\n",
    "\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"content\", \"query\"],\n",
    "    template=\"Take the context of the given pdf: {content} and must answer the following question based on it: {query}\"\n",
    ")\n",
    "\n",
    "#template.invoke({\"content\" : context, \"query\": query})\n",
    "\n",
    "# LLM model\n",
    "  \n",
    "llm = GoogleGenerativeAI(model=\"gemini-2.5-flash\", google_api_key=Secret_key)\n",
    "# messages = [\n",
    "#     (\"system\", f\"You are a helpful assistant that give answers based on the context provided.\"),\n",
    "#    (\"user\", query),\n",
    "    \n",
    "# ]\n",
    "response_llm = llm.invoke(template.format(content=context, query=query))\n",
    "\n",
    "\n",
    "# #chainining\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=template)\n",
    "\n",
    "chain.run({\"content\" : context, \"query\" : query})   \n",
    "\n",
    "print(response_llm)\n",
    "#print(db)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
